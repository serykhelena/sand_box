{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых модулей \n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Настройки для визуализации\n",
    "# Если используется темная тема - лучше текст сделать белым\n",
    "TEXT_COLOR = 'black'\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 10)\n",
    "matplotlib.rcParams['text.color'] = 'black'\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['axes.labelcolor'] = TEXT_COLOR\n",
    "matplotlib.rcParams['xtick.color'] = TEXT_COLOR\n",
    "matplotlib.rcParams['ytick.color'] = TEXT_COLOR\n",
    "\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# В функции загрузки уже есть разделение на обучение/тест\n",
    "#   воспользуемся этим на момент подготовки модели\n",
    "# Для анализа лучше посмотреть на все данные\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data type:\t<class 'list'>\n\nTarget names:\n['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n\nTarget data:\n[10  3 17  3  4 12  4 10 10 19]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим, какой у данных тип\n",
    "data = newsgroups_data['data']\n",
    "targets = newsgroups_data['target']\n",
    "target_names = newsgroups_data['target_names']\n",
    "\n",
    "print(f\"Data type:\\t{type(data)}\\n\")\n",
    "print(f\"Target names:\\n{target_names}\\n\")\n",
    "print(f\"Target data:\\n{targets[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Corpus: ['пирожок', 'пирог', 'сегодня', 'можно', 'кушать', 'ходил', 'поел', 'кино', 'лишь', 'не', 'это']\n"
     ]
    }
   ],
   "source": [
    "# Здесь специально сделана некоторая предобработка, \n",
    "#   которая обычно делается в рамках подготовки\n",
    "texts_dataset = [\n",
    "    \"пирожок это лишь пирожок\",\n",
    "    \"пирог не кушать пирожок можно\",\n",
    "    \"сегодня ходил кино поел пирог\"\n",
    "]\n",
    "\n",
    "corpus = set()\n",
    "# Для начала составим словарь\n",
    "for text in texts_dataset:\n",
    "    tokens = text.split(' ')\n",
    "    corpus.update(tokens)\n",
    "\n",
    "corpus = list(corpus)\n",
    "print(f'Corpus: {corpus}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   пирожок  пирог  сегодня  можно  кушать  ходил  поел  кино  лишь  не  это  \\\n",
       "0        2      0        0      0       0      0     0     0     1   0    1   \n",
       "1        1      1        0      1       1      0     0     0     0   1    0   \n",
       "2        0      1        1      0       0      1     1     1     0   0    0   \n",
       "\n",
       "                          _texts  \n",
       "0       пирожок это лишь пирожок  \n",
       "1  пирог не кушать пирожок можно  \n",
       "2  сегодня ходил кино поел пирог  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>пирожок</th>\n      <th>пирог</th>\n      <th>сегодня</th>\n      <th>можно</th>\n      <th>кушать</th>\n      <th>ходил</th>\n      <th>поел</th>\n      <th>кино</th>\n      <th>лишь</th>\n      <th>не</th>\n      <th>это</th>\n      <th>_texts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>пирожок это лишь пирожок</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>пирог не кушать пирожок можно</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>сегодня ходил кино поел пирог</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# После составления корпуса мы можем составить матрицу попаданий\n",
    "samples_count = len(texts_dataset)\n",
    "corpus_len = len(corpus)\n",
    "X_data = np.zeros((samples_count, corpus_len), dtype=int)\n",
    "\n",
    "for i_sample, text in enumerate(texts_dataset):\n",
    "    tokens = text.split(' ')\n",
    "    for token in tokens:\n",
    "        token_index = corpus.index(token)\n",
    "        X_data[i_sample, token_index] += 1\n",
    "\n",
    "# Для лучшего представления составим DataFrame\n",
    "X_df = pd.DataFrame(X_data, columns=corpus)\n",
    "X_df['_texts'] = texts_dataset\n",
    "\n",
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   пирожок  пирог  сегодня  можно  кушать  ходил  поел  кино  лишь   не   это  \\\n",
       "0      0.5    0.0      0.0    0.0     0.0    0.0   0.0   0.0  0.25  0.0  0.25   \n",
       "1      0.2    0.2      0.0    0.2     0.2    0.0   0.0   0.0  0.00  0.2  0.00   \n",
       "2      0.0    0.2      0.2    0.0     0.0    0.2   0.2   0.2  0.00  0.0  0.00   \n",
       "\n",
       "                          _texts  \n",
       "0       пирожок это лишь пирожок  \n",
       "1  пирог не кушать пирожок можно  \n",
       "2  сегодня ходил кино поел пирог  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>пирожок</th>\n      <th>пирог</th>\n      <th>сегодня</th>\n      <th>можно</th>\n      <th>кушать</th>\n      <th>ходил</th>\n      <th>поел</th>\n      <th>кино</th>\n      <th>лишь</th>\n      <th>не</th>\n      <th>это</th>\n      <th>_texts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>пирожок это лишь пирожок</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.2</td>\n      <td>0.00</td>\n      <td>пирог не кушать пирожок можно</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>сегодня ходил кино поел пирог</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "row_sums = X_data.sum(axis=1)\n",
    "X_data_norm = X_data/row_sums[:,None]\n",
    "\n",
    "X_df = pd.DataFrame(X_data_norm, columns=corpus)\n",
    "X_df['_texts'] = texts_dataset\n",
    "\n",
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    # Ограничение максимального кол-ва признаков (размера выходного вектора)\n",
    "    #   None -> не ограничено, вычисляется из данных\n",
    "    max_features=None,\n",
    ")\n",
    "\n",
    "X_data = [\n",
    "    \"Пирожок - это лишь пирожок!\",\n",
    "    \"Пирог не кушать, пирожок - можно.\",\n",
    "    \"Я сегодня ходил в кино и поел пирог!\"\n",
    "]\n",
    "\n",
    "X_data_vec = vectorizer.fit_transform(X_data)\n",
    "# Отобразим векторизированное представление (кол-во данных, кол-во фич)\n",
    "print(X_data_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['кино',\n",
       " 'кушать',\n",
       " 'лишь',\n",
       " 'можно',\n",
       " 'не',\n",
       " 'пирог',\n",
       " 'пирожок',\n",
       " 'поел',\n",
       " 'сегодня',\n",
       " 'ходил',\n",
       " 'это']"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "corpus = vectorizer.get_feature_names()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'кино': 1.6931471805599454,\n",
       " 'кушать': 1.6931471805599454,\n",
       " 'лишь': 1.6931471805599454,\n",
       " 'можно': 1.6931471805599454,\n",
       " 'не': 1.6931471805599454,\n",
       " 'пирог': 1.2876820724517808,\n",
       " 'пирожок': 1.2876820724517808,\n",
       " 'поел': 1.6931471805599454,\n",
       " 'сегодня': 1.6931471805599454,\n",
       " 'ходил': 1.6931471805599454,\n",
       " 'это': 1.6931471805599454}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "dict(zip(corpus, vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       кино    кушать      лишь     можно        не     пирог   пирожок  \\\n",
       "0  0.000000  0.000000  0.481482  0.000000  0.000000  0.000000  0.732359   \n",
       "1  0.000000  0.490479  0.000000  0.490479  0.490479  0.373022  0.373022   \n",
       "2  0.467351  0.000000  0.000000  0.000000  0.000000  0.355432  0.000000   \n",
       "\n",
       "       поел   сегодня     ходил       это  \\\n",
       "0  0.000000  0.000000  0.000000  0.481482   \n",
       "1  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.467351  0.467351  0.467351  0.000000   \n",
       "\n",
       "                                 _texts  \n",
       "0           Пирожок - это лишь пирожок!  \n",
       "1     Пирог не кушать, пирожок - можно.  \n",
       "2  Я сегодня ходил в кино и поел пирог!  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>кино</th>\n      <th>кушать</th>\n      <th>лишь</th>\n      <th>можно</th>\n      <th>не</th>\n      <th>пирог</th>\n      <th>пирожок</th>\n      <th>поел</th>\n      <th>сегодня</th>\n      <th>ходил</th>\n      <th>это</th>\n      <th>_texts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.481482</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.732359</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.481482</td>\n      <td>Пирожок - это лишь пирожок!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.490479</td>\n      <td>0.000000</td>\n      <td>0.490479</td>\n      <td>0.490479</td>\n      <td>0.373022</td>\n      <td>0.373022</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>Пирог не кушать, пирожок - можно.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.467351</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.355432</td>\n      <td>0.000000</td>\n      <td>0.467351</td>\n      <td>0.467351</td>\n      <td>0.467351</td>\n      <td>0.000000</td>\n      <td>Я сегодня ходил в кино и поел пирог!</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df = pd.DataFrame(X_data_vec.todense(), columns = corpus)\n",
    "df['_texts'] = X_data\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    x1 x2  y\n",
       "0    0  A  0\n",
       "1    0  B  0\n",
       "2    0  B  1\n",
       "3    0  B  1\n",
       "4    0  C  0\n",
       "5    1  A  0\n",
       "6    1  A  1\n",
       "7    1  C  1\n",
       "8    1  A  1\n",
       "9    1  A  0\n",
       "10   2  C  0\n",
       "11   2  B  1\n",
       "12   2  B  1\n",
       "13   2  C  1\n",
       "14   2  C  0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>A</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>B</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>B</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>B</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>C</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>A</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>A</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>C</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>A</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>A</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2</td>\n      <td>C</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2</td>\n      <td>B</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2</td>\n      <td>B</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2</td>\n      <td>C</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2</td>\n      <td>C</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x1': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
    "    'x2': ['A', 'B', 'B', 'B', 'C', 'A', 'A', 'C', 'A', 'A', 'C', 'B', 'B', 'C', 'C'],\n",
    "    'y': [0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0]\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгрузим данные\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train', random_state=RANDOM_STATE)\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset='test', random_state=RANDOM_STATE)\n",
    "\n",
    "X_train = newsgroups_train['data']\n",
    "y_train = newsgroups_train['target']\n",
    "\n",
    "X_test = newsgroups_test['data']\n",
    "y_test = newsgroups_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train accuracy: 0.9326498143892522\nTest accuracy: 0.7738980350504514\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Без всяких предобработок кидаем, что есть в трансформацию\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train_vec, y_train)\n",
    "\n",
    "print(f'Train accuracy: {nb_clf.score(X_train_vec, y_train)}')\n",
    "print(f'Test accuracy: {nb_clf.score(X_test_vec, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lena/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/lena/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/lena/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Скачиваем необходимые модули фреймворка nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\nSubject: SI Clock Poll - Final Call\nSummary: Final call for SI clock reports\nKeywords: SI,acceleration,clock,upgrade\nArticle-I.D.: shelley.1qvfo9INNc3s\nOrganization: University of Washington\nLines: 11\nNNTP-Posting-Host: carson.u.washington.edu\n\nA fair number of brave souls who upgraded their SI clock oscillator have\nshared their experiences for this poll. Please send a brief message detailing\nyour experiences with the procedure. Top speed attained, CPU rated speed,\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\nfunctionality with 800 and 1.4 m floppies are especially requested.\n\nI will be summarizing in the next two days, so please add to the network\nknowledge base if you have done the clock upgrade and haven't answered this\npoll. Thanks.\n\nGuy Kuo <guykuo@u.washington.edu>\n\n"
     ]
    }
   ],
   "source": [
    "# Выберем текст для примера и посмотрим\n",
    "sample_text = X_train[1]\n",
    "\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "from: guykuo@carson.u.washington.edu (guy kuo)\nsubject: si clock poll - final call\nsummary: final call for si clock reports\nkeywords: si,acceleration,clock,upgrade\narticle-i.d.: shelley.1qvfo9innc3s\norganization: university of washington\nlines: 11\nnntp-posting-host: carson.u.washington.edu\n\na fair number of brave souls who upgraded their si clock oscillator have\nshared their experiences for this poll. please send a brief message detailing\nyour experiences with the procedure. top speed attained, cpu rated speed,\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\nfunctionality with 800 and 1.4 m floppies are especially requested.\n\ni will be summarizing in the next two days, so please add to the network\nknowledge base if you have done the clock upgrade and haven't answered this\npoll. thanks.\n\nguy kuo <guykuo@u.washington.edu>\n\n"
     ]
    }
   ],
   "source": [
    "# Проверим работу приведения к нижнему регистру\n",
    "# Этот подход позволяет исключить различия слов Hello и hello,\n",
    "#   так как по сути это одно и то же слово\n",
    "sample_text = sample_text.lower()\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "from guykuocarsonuwashingtonedu guy kuo\nsubject si clock poll  final call\nsummary final call for si clock reports\nkeywords siaccelerationclockupgrade\narticleid shelley1qvfo9innc3s\norganization university of washington\nlines 11\nnntppostinghost carsonuwashingtonedu\n\na fair number of brave souls who upgraded their si clock oscillator have\nshared their experiences for this poll please send a brief message detailing\nyour experiences with the procedure top speed attained cpu rated speed\nadd on cards and adapters heat sinks hour of usage per day floppy disk\nfunctionality with 800 and 14 m floppies are especially requested\n\ni will be summarizing in the next two days so please add to the network\nknowledge base if you have done the clock upgrade and havent answered this\npoll thanks\n\nguy kuo guykuouwashingtonedu\n\n"
     ]
    }
   ],
   "source": [
    "# Удаление пунктуации\n",
    "# Более сложный анализ может учитывать пунктуацию, но для простых\n",
    "#   случаев пунктуация исключается, чтобы оставить лишь слова\n",
    "#   как основную информацию\n",
    "punct_transl = str.maketrans('', '', punctuation)\n",
    "sample_text = sample_text.translate(punct_transl)\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "from guykuocarsonuwashingtonedu guy kuo\nsubject si clock poll  final call\nsummary final call for si clock reports\nkeywords siaccelerationclockupgrade\narticleid shelleyqvfoinncs\norganization university of washington\nlines \nnntppostinghost carsonuwashingtonedu\n\na fair number of brave souls who upgraded their si clock oscillator have\nshared their experiences for this poll please send a brief message detailing\nyour experiences with the procedure top speed attained cpu rated speed\nadd on cards and adapters heat sinks hour of usage per day floppy disk\nfunctionality with  and  m floppies are especially requested\n\ni will be summarizing in the next two days so please add to the network\nknowledge base if you have done the clock upgrade and havent answered this\npoll thanks\n\nguy kuo guykuouwashingtonedu\n\n"
     ]
    }
   ],
   "source": [
    "# Удаление чисел\n",
    "# Числа как правило редко повторяются, для простого подхода\n",
    "#   достаточно удалить числа, так как это неповторяющаяся информация\n",
    "sample_text = re.sub(r'\\d+', '', sample_text)\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "from guykuocarsonuwashingtonedu guy kuo subject si clock poll final call summary final call for si clock reports keywords siaccelerationclockupgrade articleid shelleyqvfoinncs organization university of washington lines nntppostinghost carsonuwashingtonedu a fair number of brave souls who upgraded their si clock oscillator have shared their experiences for this poll please send a brief message detailing your experiences with the procedure top speed attained cpu rated speed add on cards and adapters heat sinks hour of usage per day floppy disk functionality with and m floppies are especially requested i will be summarizing in the next two days so please add to the network knowledge base if you have done the clock upgrade and havent answered this poll thanks guy kuo guykuouwashingtonedu \n"
     ]
    }
   ],
   "source": [
    "# Удаление повторяющихся пробелов\n",
    "# Часто в текста делают кучу пробелов и отступов - они не несут информации\n",
    "sample_text = re.sub(r'\\s+', ' ', sample_text)\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['from', 'guykuocarsonuwashingtonedu', 'guy', 'kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'for', 'si', 'clock', 'reports', 'keywords', 'siaccelerationclockupgrade', 'articleid', 'shelleyqvfoinncs', 'organization', 'university', 'of', 'washington', 'lines', 'nntppostinghost', 'carsonuwashingtonedu', 'a', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'si', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', 'please', 'send', 'a', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'on', 'cards', 'and', 'adapters', 'heat', 'sinks', 'hour', 'of', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'with', 'and', 'm', 'floppies', 'are', 'especially', 'requested', 'i', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', 'havent', 'answered', 'this', 'poll', 'thanks', 'guy', 'kuo', 'guykuouwashingtonedu']\n"
     ]
    }
   ],
   "source": [
    "# Токенизация - превращаем одну большую строку в массив токенов (слов)\n",
    "# Под токенами могут пониматься не только слова, но и комбинации слов,\n",
    "#   хотя для простого анализа - достаточно токенизировать до слов\n",
    "word_tokens = nltk.word_tokenize(sample_text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'i', 'those', 'too', 'couldn', 've', 'how', \"you'd\", 'y', 'or', 'don', 'was', 'you', 'me', 'be', 'at', 'through', 'wouldn', 'o', 'do', 'after', \"mustn't\", 'from', \"doesn't\", 'same', 'wasn', 'then', 'being', 'she', 'ma', 'ourselves', 'herself', \"it's\", 'about', \"haven't\", 'having', 'it', 'only', 'by', \"won't\", 'below', \"you've\", 'yours', 'before', 'where', 'doing', 'than', 'your', 'the', 'other', 'few', 'most', 'yourselves', 'as', \"shouldn't\", 'no', 'they', 'themselves', 'didn', \"weren't\", 'but', 'itself', \"wouldn't\", \"you'll\", 'm', 'why', 'here', 'theirs', 'between', 'for', 'over', 'him', \"she's\", 'd', 't', 'these', 'aren', 're', 'this', 'on', 'to', \"hadn't\", 'and', 'such', 'above', 'of', 'during', 'just', \"needn't\", 'does', 'did', 'yourself', 'can', 'some', \"wasn't\", 'their', 'against', 'ours', 'because', 'both', \"mightn't\", 'hadn', 'out', 'until', 'under', \"you're\", 'myself', 'in', 'an', 'should', 'not', 'had', 'each', 'shan', 'which', 'my', 'mightn', 'hasn', 'down', 'so', \"should've\", \"hasn't\", 'who', \"isn't\", \"don't\", 'own', 'all', 'what', 'if', 'very', 'we', 'ain', 'll', 'are', 'been', 'himself', 'off', 'that', 'there', \"aren't\", 'them', 'nor', 'needn', 'its', 'isn', 'have', 'into', 'now', 'whom', 'further', \"shan't\", 'am', 'when', 'haven', 'more', 'while', \"couldn't\", 's', 'mustn', 'once', 'with', 'won', 'is', 'our', 'has', 'shouldn', 'were', 'will', 'up', 'weren', 'a', \"didn't\", 'doesn', 'any', 'he', 'again', 'hers', 'his', 'her', \"that'll\"}\n"
     ]
    }
   ],
   "source": [
    "# Удаляем стоп-слова, для начала посмотрим, что это за слова\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['guykuocarsonuwashingtonedu', 'guy', 'kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'si', 'clock', 'reports', 'keywords', 'siaccelerationclockupgrade', 'articleid', 'shelleyqvfoinncs', 'organization', 'university', 'washington', 'lines', 'nntppostinghost', 'carsonuwashingtonedu', 'fair', 'number', 'brave', 'souls', 'upgraded', 'si', 'clock', 'oscillator', 'shared', 'experiences', 'poll', 'please', 'send', 'brief', 'message', 'detailing', 'experiences', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'cards', 'adapters', 'heat', 'sinks', 'hour', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'floppies', 'especially', 'requested', 'summarizing', 'next', 'two', 'days', 'please', 'add', 'network', 'knowledge', 'base', 'done', 'clock', 'upgrade', 'havent', 'answered', 'poll', 'thanks', 'guy', 'kuo', 'guykuouwashingtonedu']\n"
     ]
    }
   ],
   "source": [
    "# Теперь фильтруем стоп-слова из наших токенов\n",
    "word_tokens = [word for word in word_tokens if word not in stop_words]\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['guykuocarsonuwashingtonedu', 'guy', 'kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'si', 'clock', 'report', 'keywords', 'siaccelerationclockupgrade', 'articleid', 'shelleyqvfoinncs', 'organization', 'university', 'washington', 'line', 'nntppostinghost', 'carsonuwashingtonedu', 'fair', 'number', 'brave', 'soul', 'upgraded', 'si', 'clock', 'oscillator', 'shared', 'experience', 'poll', 'please', 'send', 'brief', 'message', 'detailing', 'experience', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'card', 'adapter', 'heat', 'sink', 'hour', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'floppy', 'especially', 'requested', 'summarizing', 'next', 'two', 'day', 'please', 'add', 'network', 'knowledge', 'base', 'done', 'clock', 'upgrade', 'havent', 'answered', 'poll', 'thanks', 'guy', 'kuo', 'guykuouwashingtonedu']\n"
     ]
    }
   ],
   "source": [
    "# Проводим лемматизацию - приводим к нормальной форме\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_tokens = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bat\nare\nfoot\n"
     ]
    }
   ],
   "source": [
    "# Чтобы лучше понять, как он работает - рассмотрим примеры:\n",
    "print(wordnet_lemmatizer.lemmatize('bats'))\n",
    "print(wordnet_lemmatizer.lemmatize('are'))  # Ууупс, тут не преобразовалось в be =(\n",
    "print(wordnet_lemmatizer.lemmatize('feet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "guykuocarsonuwashingtonedu guy kuo subject si clock poll final call summary final call si clock report keywords siaccelerationclockupgrade articleid shelleyqvfoinncs organization university washington line nntppostinghost carsonuwashingtonedu fair number brave soul upgraded si clock oscillator shared experience poll please send brief message detailing experience procedure top speed attained cpu rated speed add card adapter heat sink hour usage per day floppy disk functionality floppy especially requested summarizing next two day please add network knowledge base done clock upgrade havent answered poll thanks guy kuo guykuouwashingtonedu\n"
     ]
    }
   ],
   "source": [
    "# После этого нам нужно объединить токены обратно в единую строку \n",
    "#   для будущего кодирования\n",
    "processed_text = ' '.join(word_tokens)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}